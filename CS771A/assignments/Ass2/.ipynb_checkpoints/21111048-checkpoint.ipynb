{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d8523b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63467f58",
   "metadata": {},
   "source": [
    "# Q1.a)  Use this function to find minima for (i) x2 + 3x+4 and (ii) x4 â€“ 3x2 +2x. [5 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d38eb88",
   "metadata": {},
   "source": [
    "### 1. $x^2+3x+4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a94b4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=np.linspace(-15,15,300)\n",
    "y=x**2+3*x+4\n",
    "plt.xlim([-15,15])\n",
    "plt.ylim([0,10])\n",
    "plt.grid()\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87891b3",
   "metadata": {},
   "source": [
    "## 2. $x^4 - 3x^2 + 2x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bbfee287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=np.linspace(-15,15,300)\n",
    "y=x**4-3*x*x+2*x\n",
    "plt.xlim([-15,15])\n",
    "plt.ylim([-10,15])\n",
    "plt.grid()\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2232a4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 64\n",
      "Minima for the function x^2+3x+4 is found at -1.5 and the minimum value of the function is 1.75\n",
      "Iterations: 66\n",
      "Minima for the function x^4-3x^2+2x is found at -1.366 and the minimum value of the function is -4.848076206064\n"
     ]
    }
   ],
   "source": [
    "gradient1 = lambda x: 2*x+3 ## specifies gradient for the function x2 + 3x+4\n",
    "gradient2 = lambda x: 4*(x**3)-6*x+2 ## specifies gradient for the function  x4 â€“ 3x2 +2x.\n",
    "\n",
    "def gradient_descent (gradient, init_, learn_rate, n_iter=1000, tol=1e-06):\n",
    "    x = init_\n",
    "    for i in range (n_iter): \n",
    "        delta = (-1)*learn_rate * gradient(x)\n",
    "        if np.all(np.abs (delta) <= tol): \n",
    "            break \n",
    "        x += delta \n",
    "    print('Iterations:', str(i+1))\n",
    "    return  round(x*1000)/1000\n",
    "\n",
    "x1 = gradient_descent(gradient1,4 , 0.1)\n",
    "min_func1 = x1**2+3*x1+4\n",
    "print('Minima for the function x^2+3x+4 is found at '+ str(x1), 'and the minimum value of the function is', str(min_func1))\n",
    "\n",
    "x2 = gradient_descent(gradient2,-1 , 0.01)\n",
    "min_func2 = x2**4 - 3*(x2**2) + 2*x2\n",
    "print('Minima for the function x^4-3x^2+2x is found at '+ str(x2), 'and the minimum value of the function is', str(min_func2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a2d8a1",
   "metadata": {},
   "source": [
    "## NOTE: The first polynomial is a convex function, so initialisation won't matter a lot. But the second polynomial is a non convex function, and hence the initialization value matters. I tried initialising with the value **~4**, and ended up finding local minima. Finally, the initial value of **~-1** is giving me the global minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91d3e9b",
   "metadata": {},
   "source": [
    "# Q1.b) Write a gradient function to calculate gradients for a linear regression y = ax + b [10 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74212e1",
   "metadata": {},
   "source": [
    "The gradients w.r.t __a__ and __b__ are calculated as: \n",
    "$$\\nabla_a = \\frac{-2}{n} * \\sum(X * (Y - Y_{pred}))$$\n",
    "$$\\nabla_b = \\frac{-2}{n} * \\sum(Y - Y_pred)$$\n",
    "<br>\n",
    "\n",
    "$where, \\; Y_{pred} = a\\ast X + b$,<br> n = number of points in the dataset, <br>X is the input vector, <br>Y is the output vector \n",
    "\n",
    "### The same are used in the function below to elaborate their actual usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46c907fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradients(X, Y):\n",
    "    Y_pred = a*X + b \n",
    "        \n",
    "    D_a = (-2/n) * np.dot(X.T,(Y - Y_pred)) # gradient wrt a \n",
    "    D_b = (-2/n) * sum(Y - Y_pred) # gradient wrt b\n",
    "    return D_a, D_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776bf08e",
   "metadata": {},
   "source": [
    "# Q1.c) Generate artificial data for this regression according to the following protocol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "025d9c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the parameters to be used later\n",
    "my_iter = 1000\n",
    "my_tol = 1e-06\n",
    "my_lr = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8717fbb2",
   "metadata": {},
   "source": [
    "### Here we are trying to generate some artificial data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a92cea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating some random data \n",
    "\n",
    "np.random.seed (0)\n",
    "X = 2.5*np.random.randn(10000)+1.5 #Ar\n",
    "res=1.5*np.random.randn(10000) # Gene.\n",
    "Y = 2+0.3*X + res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded139b9",
   "metadata": {},
   "source": [
    "##  Here we are trying to calculate partial derivatives in each iteration, w.r.t to the free parameteres __a__ and __b__. We updated their values based on the learning rate and the gradients calculated, and break the loop under following two scenarios:\n",
    "### 1. Iterations have equalled to max_iter\n",
    "### 2. Delta_a and delta_b have dropped below a specified tolerance, $10^{-6}$, in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5900b39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using batch gradient descent with a learning rate of 0.01, the values of (a,b, execution time, #iterations) are  (0.3, 2.02, 1.1576311588287354, 1000)\n"
     ]
    }
   ],
   "source": [
    "batch_time = 0\n",
    "def lr_gd(X, Y, learning_rate = my_lr, init_a = 0, init_b = 0, max_iter = my_iter, tol = my_tol):\n",
    "    start_time = time.time()\n",
    "\n",
    "    a = 0\n",
    "    b = 0\n",
    "\n",
    "    n = X.shape[0] \n",
    "    \n",
    "    for i in range(max_iter): \n",
    "        Y_pred = a*X + b \n",
    "        \n",
    "        D_a = (-2/n) * np.dot(X.T,(Y - Y_pred))  \n",
    "        D_b = (-2/n) * sum(Y - Y_pred)  \n",
    "        \n",
    "        delta_a = (-1)*learning_rate * D_a\n",
    "        delta_b = (-1)*learning_rate * D_b\n",
    "\n",
    "        if (np.all(np.abs (delta_a) <= tol) & np.all(np.abs (delta_b) <= tol)): \n",
    "            break\n",
    "        \n",
    "        a+=delta_a\n",
    "        b+=delta_b\n",
    "# =============================================================================\n",
    "#         print(i)\n",
    "# =============================================================================\n",
    "    #print('It took ', str(i+1), ' iterations to arrive at the desired result')\n",
    "    #batch_time = time.time() - start_time\n",
    "    #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    return round(a,2), round(b,2),time.time() - start_time, i+1 \n",
    "\n",
    "a,b,batch_time, iterations = lr_gd(X, Y)\n",
    "print('\\nUsing batch gradient descent with a learning rate of 0.01, the values of (a,b, execution time, #iterations) are ', (a,b,batch_time, iterations))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d353060",
   "metadata": {},
   "source": [
    "# Q1.d) Implement minibatch stochastic gradient descent using the code base you have developed so far.[15 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53555593",
   "metadata": {},
   "source": [
    "## Stochastic- Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51b202b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_stochastic_gd(X, Y, learning_rate = my_lr, init_a = 0, init_b = 0, max_iter = my_iter, tol = my_tol):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    a = 0\n",
    "    b = 0\n",
    "\n",
    "    n = X.shape[0] \n",
    "    \n",
    "    for i in range(max_iter): \n",
    "        #np.random.seed (5)\n",
    "        random_index=np.random.randint(0,n-1)\n",
    "        X_mini= X[random_index]\n",
    "        Y_mini = Y[random_index]\n",
    "        \n",
    "        Y_pred = a*X_mini + b \n",
    "\n",
    "       # print(X_mini, Y_mini)\n",
    "\n",
    "\n",
    "        D_a = (-2)*X_mini*(Y_mini - Y_pred)  \n",
    "        D_b = (-2)*(Y_mini - Y_pred)  \n",
    "\n",
    "        delta_a = (-1)*learning_rate * D_a\n",
    "        delta_b = (-1)*learning_rate * D_b\n",
    "\n",
    "        if (np.all(np.abs (delta_a) <= tol) & np.all(np.abs (delta_b) <= tol)): \n",
    "            break\n",
    "\n",
    "        a+=delta_a\n",
    "        b+=delta_b\n",
    "\n",
    "# =============================================================================\n",
    "#         print(i)\n",
    "# =============================================================================\n",
    "    #print('It took ', str(i+1), ' iterations to arrive at the desired result')\n",
    "    #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    #print (round(a,2), round(b,2))\n",
    "    return str(i+1), time.time() - start_time, round(a,2), round(b,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c74594",
   "metadata": {},
   "source": [
    "## Since there is a large amount of fluctuation in SGD, we will try to run the SGD method multiple times, and stop when we get the values of our free parameters, __a__ and __b__ in our desired range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f0a9f",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ca0f53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0.028056621551513672 0.34 2.0\n",
      "1000 0.022626399993896484 0.36 1.99\n",
      "1000 0.022193431854248047 0.47 2.18\n",
      "1000 0.023459196090698242 0.1 2.01\n",
      "1000 0.02993941307067871 0.26 2.06\n",
      "1000 0.023894071578979492 0.42 2.06\n",
      "1000 0.020761966705322266 0.23 1.93\n",
      "685 0.025142192840576172 0.35 1.88\n",
      "1000 0.029018640518188477 0.37 1.97\n",
      "1000 0.021324634552001953 0.24 1.95\n",
      "1000 0.02483963966369629 0.11 1.96\n",
      "1000 0.028622150421142578 0.36 2.11\n",
      "1000 0.022945642471313477 0.21 2.0\n",
      "1000 0.024069786071777344 0.29 2.02\n",
      "1000 0.028271198272705078 0.21 2.27\n",
      "1000 0.028585433959960938 0.15 1.99\n",
      "1000 0.022594213485717773 0.31 2.03\n",
      "1000 0.029275178909301758 0.26 1.98\n",
      "1000 0.02408885955810547 0.24 2.11\n",
      "928 0.023853063583374023 0.43 2.07\n",
      "1000 0.03603529930114746 0.38 2.35\n",
      "1000 0.023396730422973633 0.33 2.13\n",
      "1000 0.04725503921508789 0.45 1.97\n",
      "1000 0.024576902389526367 0.25 2.12\n",
      "1000 0.021357297897338867 0.27 2.08\n",
      "1000 0.02375197410583496 0.39 1.98\n",
      "1000 0.03796720504760742 0.4 1.97\n",
      "1000 0.038022756576538086 0.38 2.05\n",
      "1000 0.05794501304626465 0.26 1.92\n",
      "1000 0.029488563537597656 0.22 2.12\n",
      "1000 0.06354546546936035 0.36 2.04\n",
      "1000 0.06001758575439453 0.23 1.92\n",
      "1000 0.047751665115356445 0.22 2.01\n",
      "1000 0.05182480812072754 0.23 2.17\n",
      "1000 0.046797752380371094 0.47 1.97\n",
      "1000 0.037824153900146484 0.36 2.06\n",
      "1000 0.024018049240112305 0.53 1.94\n",
      "1000 0.022814512252807617 0.35 2.33\n",
      "1000 0.02353048324584961 0.19 2.0\n",
      "1000 0.029903650283813477 0.43 1.93\n",
      "1000 0.02201056480407715 0.49 2.15\n",
      "1000 0.022578001022338867 0.24 2.07\n",
      "1000 0.020388126373291016 0.24 2.08\n",
      "1000 0.024708986282348633 0.51 2.12\n",
      "1000 0.02426457405090332 0.3 2.11\n",
      "1000 0.03115248680114746 0.21 2.18\n",
      "1000 0.023274898529052734 0.26 2.03\n",
      "1000 0.022557735443115234 0.28 1.89\n",
      "1000 0.02415633201599121 0.45 1.99\n",
      "1000 0.026940584182739258 0.36 1.95\n",
      "1000 0.022688627243041992 0.23 2.15\n",
      "1000 0.024911165237426758 0.13 2.17\n",
      "783 0.017281293869018555 0.33 2.02\n",
      "1000 0.031915903091430664 0.36 1.95\n",
      "1000 0.02756214141845703 0.42 2.04\n",
      "1000 0.022447824478149414 0.23 2.1\n",
      "1000 0.0201568603515625 0.39 2.03\n",
      "1000 0.022179841995239258 0.39 2.11\n",
      "1000 0.021310091018676758 0.4 1.88\n",
      "1000 0.0446779727935791 0.31 2.19\n",
      "1000 0.038971900939941406 0.38 1.93\n",
      "1000 0.03081679344177246 0.33 2.03\n",
      "1000 0.04577827453613281 0.35 2.02\n",
      "1000 0.0610661506652832 0.37 2.13\n",
      "1000 0.05734395980834961 0.22 2.01\n",
      "1000 0.03350424766540527 0.19 2.07\n",
      "402 0.015336275100708008 0.38 2.02\n",
      "1000 0.036777496337890625 0.26 1.9\n",
      "1000 0.02961421012878418 0.39 1.97\n",
      "1000 0.021460771560668945 0.38 1.93\n",
      "1000 0.023394346237182617 0.03 2.12\n",
      "1000 0.026942968368530273 0.33 2.01\n",
      "1000 0.03283286094665527 0.16 2.05\n",
      "1000 0.044587135314941406 0.15 2.1\n",
      "1000 0.02848362922668457 0.4 1.96\n",
      "1000 0.02747488021850586 0.31 2.05\n",
      "1000 0.022762537002563477 0.2 2.0\n",
      "1000 0.021873950958251953 0.32 2.08\n",
      "1000 0.03040289878845215 0.33 2.17\n",
      "1000 0.02463984489440918 0.23 2.28\n",
      "1000 0.023595809936523438 0.3 2.01\n",
      "1000 0.023595809936523438\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    it, exec_time, a,b = lr_stochastic_gd(X, Y)\n",
    "    print(it, exec_time, a, b)\n",
    "    if((a in [0.29, 0.3, 0.31]) and (b in [1.99, 2.0, 2.01])):\n",
    "        break\n",
    "\n",
    "print(it, exec_time)\n",
    "sgd_time = exec_time\n",
    "sgd_iterations = it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9380266",
   "metadata": {},
   "source": [
    "## Here we are creating a function for calculating the LR parameters using Minibatch GD\n",
    "### Steps:\n",
    "1. For each iteration, we are sampling the data from our input dataset of size equal to the batch size provided\n",
    "2. We will calculate the partial derivatives wrt __a__ and __b__\n",
    "3. We will update the values of __a__ and __b__ and calulate the delta of these params\n",
    "4. We will check if the delta values so calcuted fall below the pre-decided tolerance level\n",
    "5. If 4 happens to be true, we will stop and report the values of __a__ and __b__ \n",
    "6. If 4 is false, we will iterate till max_iter is reached, and return the values of __a__ and __b__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e2fb424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_mb_gd(X, Y, batch_size, learning_rate = my_lr, init_a = 0, init_b = 0, max_iter = my_iter, tol = my_tol):\n",
    "    start_time = time.time()\n",
    "    a = 0\n",
    "    b = 0\n",
    "\n",
    "    n = float(len(X)) \n",
    "    \n",
    "    for i in range(max_iter): \n",
    "        #np.random.seed (5)\n",
    "        random_index=np.random.randint(0,n-batch_size-1)\n",
    "        X_mini= X[random_index:random_index+batch_size]\n",
    "        Y_mini = Y[random_index:random_index+batch_size]\n",
    "        \n",
    "        n_tmp = len(X_mini)\n",
    "        Y_pred = a*X_mini + b \n",
    "\n",
    "        D_a = (-2/n_tmp) * np.dot(X_mini.T,(Y_mini - Y_pred))  \n",
    "        D_b = (-2/n_tmp) * sum(Y_mini - Y_pred)  \n",
    "\n",
    "        delta_a = (-1)*learning_rate * D_a\n",
    "        delta_b = (-1)*learning_rate * D_b\n",
    "\n",
    "        if (np.all(np.abs (delta_a) <= tol) & np.all(np.abs (delta_b) <= tol)): \n",
    "            break\n",
    "\n",
    "        a+=delta_a\n",
    "        b+=delta_b\n",
    "\n",
    "# =============================================================================\n",
    "#         print(i)\n",
    "# =============================================================================\n",
    "    #print('It took ', str(i+1), ' iterations to arrive at the desired result')\n",
    "    #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    #print (round(a,2), round(b,2))\n",
    "    return str(i+1), time.time() - start_time, round(a,2), round(b,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a7c3b4",
   "metadata": {},
   "source": [
    "### Now we will calculate the number of iterations, execution time for batch sizes in the range, \n",
    "## $$2^1, 2^2\\; ..... \\; 2^{10}$$\n",
    "\n",
    "### and try to find the values which give the most optimum values of __a__ and __b__ \n",
    "\n",
    "### NOTE: We are iterating over powers of 2 because it is __computationally faster__. Also powers of 2 fit the memory requirements of CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c65aaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 2, 'iterations': '1000', 'exec_time': 0.03710007667541504, 'a': 0.36, 'b': 2.13}\n",
      "{'batch_size': 4, 'iterations': '1000', 'exec_time': 0.03418135643005371, 'a': 0.28, 'b': 1.98}\n",
      "{'batch_size': 8, 'iterations': '1000', 'exec_time': 0.04057145118713379, 'a': 0.32, 'b': 1.99}\n",
      "{'batch_size': 16, 'iterations': '1000', 'exec_time': 0.04764676094055176, 'a': 0.28, 'b': 2.01}\n",
      "{'batch_size': 32, 'iterations': '1000', 'exec_time': 0.04814934730529785, 'a': 0.24, 'b': 2.01}\n",
      "{'batch_size': 64, 'iterations': '1000', 'exec_time': 0.042153358459472656, 'a': 0.29, 'b': 2.02}\n",
      "{'batch_size': 128, 'iterations': '1000', 'exec_time': 0.054213523864746094, 'a': 0.3, 'b': 2.0}\n",
      "{'batch_size': 256, 'iterations': '1000', 'exec_time': 0.06735849380493164, 'a': 0.29, 'b': 2.03}\n",
      "{'batch_size': 512, 'iterations': '1000', 'exec_time': 0.10614824295043945, 'a': 0.29, 'b': 2.02}\n",
      "{'batch_size': 1024, 'iterations': '1000', 'exec_time': 0.15604925155639648, 'a': 0.29, 'b': 2.01}\n",
      "   batch_size iterations  exec_time     a     b\n",
      "0           2       1000   0.037100  0.36  2.13\n",
      "1           4       1000   0.034181  0.28  1.98\n",
      "2           8       1000   0.040571  0.32  1.99\n",
      "3          16       1000   0.047647  0.28  2.01\n",
      "4          32       1000   0.048149  0.24  2.01\n",
      "5          64       1000   0.042153  0.29  2.02\n",
      "6         128       1000   0.054214  0.30  2.00\n",
      "7         256       1000   0.067358  0.29  2.03\n",
      "8         512       1000   0.106148  0.29  2.02\n",
      "9        1024       1000   0.156049  0.29  2.01\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [2**i for i in range(1,11)]\n",
    "out = []\n",
    "\n",
    "for b in batch_sizes:\n",
    "    #print('batch_size ', str(b))\n",
    "    tmp = {}\n",
    "    tmp['batch_size'] = b\n",
    "    tmp['iterations'], tmp['exec_time'], tmp['a'], tmp['b'] = lr_mb_gd(X, Y, b)\n",
    "    #print('\\n')\n",
    "    print(tmp)\n",
    "    out.append(tmp)\n",
    "df = pd.DataFrame(out)\n",
    "print(df.head(50))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef8320a",
   "metadata": {},
   "source": [
    "## As observed, we are getting the most optimal values for a and b using a batch size and time taken(seconds) as under:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cce392fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.054213523864746094 128 1000\n"
     ]
    }
   ],
   "source": [
    "mgd_time = df[(df['a'].isin([0.30,0.31,0.29])) & (df['b'].isin([1.99, 2.0, 2.01]))]['exec_time'].min()\n",
    "mgd_batch_size = list(df[df['exec_time'] == mgd_time]['batch_size'])[0]\n",
    "mgd_iterations = list(df[df['exec_time'] == mgd_time]['iterations'])[0]\n",
    "\n",
    "print(mgd_time,mgd_batch_size,mgd_iterations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998e5ecb",
   "metadata": {},
   "source": [
    "# Q1.e) Does SGD do better or worse in terms of time performance on our data? Is there an optimal minibatch size that works best? Quantify and interpret your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce92037",
   "metadata": {},
   "source": [
    "# Inference: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9a28c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1576311588287354 0.054213523864746094 0.023595809936523438\n"
     ]
    }
   ],
   "source": [
    "print(batch_time, mgd_time, sgd_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d507d712",
   "metadata": {},
   "source": [
    "## With fixed values for parameters like **learn rate = 0.005, and max_iter = 1000**, we can observe the time taken by the above three algorithms as:\n",
    "## $$Batch\\; Gradient\\; Descent > MiniBatch\\; Gradient\\; Descent > Stochastic\\; Gradient\\; Descent$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aa1fec",
   "metadata": {},
   "source": [
    "## The percentage reduction in execution time of Minibatch GD versus Batch GD is as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a64ed629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MGD Time_reduction_perc versus Batch GD = 95.32 %\n"
     ]
    }
   ],
   "source": [
    "print('MGD Time_reduction_perc versus Batch GD =',round(((batch_time - mgd_time)/batch_time)*100,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b635f759",
   "metadata": {},
   "source": [
    "## The percentage reduction in execution time of Stochastic GD versus Batch GD is as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fe1c16bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Time_reduction_perc versus Batch GD = 97.96 %\n"
     ]
    }
   ],
   "source": [
    "print('SGD Time_reduction_perc versus Batch GD =',round(((batch_time - sgd_time)/batch_time)*100,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b72f7c",
   "metadata": {},
   "source": [
    "## The percentage reduction in execution time of Stochastic GD versus Minibatch GD is as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3ae217ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Time_reduction_perc versus MGD = 56.48 %\n"
     ]
    }
   ],
   "source": [
    "print('SGD Time_reduction_perc versus MGD =',round(((mgd_time - sgd_time)/mgd_time)*100,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b3741",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## The most optimal batch size reported so for for Minibatch Grdadient Descent is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12232941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "print(mgd_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5119cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2920f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b12924f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "878e7e47",
   "metadata": {},
   "source": [
    "# Q2. Surprise! This problem too builds on a problem that I asked in the mid-sem exam. Consider again this Bayesian network and calculate:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe10015",
   "metadata": {},
   "source": [
    "### (i) the probability that someone has both cold and a fever [5 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d25c773",
   "metadata": {},
   "source": [
    "$$ P(cold \\cap fever)  = P(fever / cold) * P(cold) \n",
    "                       = 0.307*0.02\n",
    "                       = 0.00614 --------ANS.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe08d20c",
   "metadata": {},
   "source": [
    "### (ii) the probability that someone who has a cough has a cold. [10 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e690995",
   "metadata": {},
   "source": [
    "$$ P(cold/cough) = P(cough \\cap cold)/P(cough) ------- (0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d70f71",
   "metadata": {},
   "source": [
    "$$P(lungDisease) = P(lungDisease/smokes)*P(smokes) + P(lungDisease/not smokes) * P(not smokes)$$\n",
    "$$0.2*0.1009 + 0.8*0.001 = 0.02098 -------(1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd96e762",
   "metadata": {},
   "source": [
    "$$P(cold) = P(lungDisease)*P(cold)*P(cough/lungDisease \\cap cold) + P(lungDisease)*P(\\overline{cold})*P(cough/lungDisease \\cap\\overline{cold}) + P(\\overline{lungDisease})*P(cold)*P(cough/\\overline{lungDisease} \\cap cold) + P(\\overline{lungDisease})*P(\\overline{cold})*P(cough/\\overline{lungDisease} \\cap \\overline{cold}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7e7c5d",
   "metadata": {},
   "source": [
    "$$= 0.02098*0.02*0.7525 + 0.02098*0.98*0.505 + 0.97902*0.02*0.505 +  0.97902*0.98*0.01 $$\n",
    "$$ =0. 000315749 + 0.010383002 + 0.009888102 + 0.009594396 $$\n",
    "$$ = 0.030181249 \\;-------------(2)$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370c179d",
   "metadata": {},
   "source": [
    "$$P(cough \\cap cold) = P(lungDisease)*P(cold)*P(cough/lungDisease \\cap cold) +  P(\\overline{lungDisease})*P(cold)*P(cough/\\overline{lungDisease} \\cap cold)$$\n",
    "$$ = 0.02098*0.02*0.7525 + 0.97902*0.02*0.01 = 0.000315749 +0.009888102 = 0.010203851 ------------(3)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295b0772",
   "metadata": {},
   "source": [
    "Putting  (2) and (3) in (0), we get, \n",
    "$$0.010203851/0.030181249 = 0.33809------------ANS.$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de45bd3f",
   "metadata": {},
   "source": [
    "# Q3. Derive the MLE for the parameters of a k-sided multinomial distribution. [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20268efd",
   "metadata": {},
   "source": [
    "\n",
    "$X$ :  Random variable specifying the outcome for each possible outcomes  \n",
    "$X : {X_1 , X_2, X_3 \\ldots X_k}$ \n",
    "<br>\n",
    "<br>\n",
    "where, \n",
    "<br>\n",
    "$X_1$ denotes first possible outcome<br>\n",
    "$X_2$ denotes second possible outcome<br>\n",
    "$\\vdots$<br>\n",
    "$X_k$ denotes kth possible outcome<br>\n",
    "\n",
    "$x_1$ denotes the number of times first possible outcome occurs<br>\n",
    "$x_2$ denotes the number of times second possible outcome occurs<br>\n",
    "$\\vdots$<br>\n",
    "$x_k$ denotes the number of times kth possible outcome occurs<br>\n",
    "\n",
    "### Below is ths standard formula for pdf for multinomial distribution:\n",
    "\n",
    "$$P(X_1=x_1, X_2=x_2 .... X_k=x_k) = \\frac{n_1!}{{x_1!}{x_2!}....{x_k!}}{{\\alpha_1^{x_1}}{\\alpha_2^{x_2}}...{\\alpha_k^{x_k}}} ............ (1)$$\n",
    "\n",
    "\n",
    "and <br>\n",
    "$\\alpha_1$ : probability of having first outcome <br>\n",
    "$\\alpha_2$ : probability of having second outcome <br>\n",
    "$\\alpha_3$ : probability of having third outcome <br>\n",
    "$\\vdots$ <br>\n",
    "$\\alpha_k$ : probability of having kth outcome <br>\n",
    "$\\therefore \\alpha_1 + \\alpha_2 + \\alpha_3 + \\cdots + \\alpha_k = 1$ <br>\n",
    "\n",
    "$$\\sum{x_i}=n, \\sum{\\alpha_i}=n$$\n",
    "$$L(x_1, x_2, ..., x_k; \\alpha_1, \\alpha_2...., \\alpha_k) = L(\\alpha) = \\frac{n_1!}{{x_1!}{x_2!}....{x_k!}}{{\\alpha_1^{x_1}}{\\alpha_2^{x_2}}...{\\alpha_k^{x_k}}}$$\n",
    "\n",
    "$L(\\alpha)$ represents the likelihood <br>\n",
    "$$l(\\alpha) = \\log(L(\\alpha))$$\n",
    "$l(\\alpha)$ represents the log likelihood <br>\n",
    "$$\\log(\\frac{n_1!}{{x_1!}{x_2!}....{x_k!}}{{\\alpha_1^{x_1}}{\\alpha_2^{x_2}}...{\\alpha_k^{x_k}}})$$\n",
    "$$\\log(n!)+ \\sum_{i=1}^k{x_i\\log(\\alpha_i)} - \\sum_{i=1}^k\\log(x_i!)$$\n",
    "\n",
    "We will add a parameter $\\lambda$, to avoid $\\alpha$ taking large values <br>\n",
    "$\\lambda$: Langrangian parameter for regularization <br>\n",
    "\n",
    "So the equation now becomes $ Regularized{\\;} Log{\\;} Likelihood{\\;} \\lambda \\to RLL(\\theta,\\alpha)$<br>\n",
    "\n",
    "$$l(\\alpha, \\lambda) = L(\\alpha) + \\lambda(1-\\sum_{i=1}^k\\alpha_i) ........... (2)$$\n",
    "\n",
    "The task is to find the minima of the above function:\n",
    "\n",
    "$$\\frac{\\partial l(\\alpha, \\lambda)}{\\partial \\alpha_1} = \\frac{\\partial L(\\alpha)}{\\partial \\alpha_1} + \\frac{\\partial \\lambda(1-\\sum_{i=1}^k\\alpha_i)}{\\partial \\alpha_1}$$\n",
    "$$ = \\frac{\\partial \\log(n!)+ \\sum_{i=1}^k{x_i\\log(\\alpha_i)} - \\sum_{i=1}^k\\log(x_i!)}{\\partial \\alpha_1} + \\frac{\\partial \\lambda(1-\\sum_{i=1}^k\\alpha_i)}{\\partial \\alpha_1}$$ \n",
    "$$= 0+\\frac{\\alpha_1}{x_1} -0+0-\\lambda = \\frac{\\alpha_1}{x_1}-\\lambda$$\n",
    "<br>\n",
    "Similarly, we can show that, \n",
    "\n",
    "$$\\frac{\\partial l(\\alpha, \\lambda)}{\\partial \\alpha_k} = \\frac{\\alpha_k}{x_k}-\\lambda $$\n",
    "\n",
    "<br>\n",
    "In order to find the maxima, we need to equate these gradients to 0:\n",
    "\n",
    "$$\\frac{x_1}{\\alpha_1} = \\lambda$$\n",
    "i.e.\n",
    "$$\\alpha_1 = \\frac{x_1}{\\lambda}$$\n",
    "$$\\alpha_2 = \\frac{x_2}{\\lambda}$$\n",
    ".\n",
    ".\n",
    ".\n",
    "$$\\alpha_k = \\frac{x_k}{\\lambda} ............ (4)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cb2ed0",
   "metadata": {},
   "source": [
    "$$\\to \\alpha_1+\\alpha_2...\\alpha_k = \\frac{x_1}{\\lambda} + \\frac{x_2}{\\lambda} .... + \\frac{x_k}{\\lambda}....(3)$$\n",
    "<br>\n",
    "\n",
    "$$Since,\\;\\; \\sum(\\alpha_i) = 1, (3)\\; now\\; becomes, $$\n",
    "\n",
    "$$\\to 1 = \\frac{1}{\\lambda}(x_1+x_2+ .... x_k)$$\n",
    "$$\\to 1 = \\frac{\\sum(x_i)}{\\lambda}$$\n",
    "## $$\\to\\lambda = n -----------(5)$$\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "Differentiating again in order to get the maxima, we will obtain a Hessian matrix of the form:\n",
    "$$\\frac{\\partial^2l(\\alpha, \\lambda)}{\\partial \\alpha^2} = \\begin{bmatrix}\n",
    " \\frac{\\partial^2l(\\alpha, \\lambda)}{\\partial \\alpha_1^2} & \\frac{\\partial^2l(\\alpha, \\lambda)}{\\partial \\alpha_1\\partial \\alpha_2} & ...\\frac{\\partial^2l(\\alpha, \\lambda)}{\\partial \\alpha_1\\partial \\alpha_k} \\\\\n",
    " \\frac{\\partial^2l(\\alpha, \\lambda)}{\\partial \\alpha_2\\partial \\alpha_1} & \\frac{\\partial^2l(\\alpha, \\lambda)}{\\partial \\alpha_2^2} & ...\\frac{\\partial^2l(\\alpha, \\lambda)}{\\partial \\alpha_2\\partial \\alpha_k}\\\\\n",
    "  .& & \\\\\n",
    "  .& & \\\\\n",
    "  .& & \\\\\n",
    " \\frac{\\partial^2l(\\alpha, \\lambda)}{\\partial \\alpha_k\\partial \\alpha_1} & \\frac{\\partial^2l(\\alpha, \\lambda)}{\\partial \\alpha_k\\partial \\alpha_2} & ...\\frac{\\partial^2l(\\alpha, \\lambda)}{\\partial \\alpha_k^2}\\\\\n",
    "\\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ee773a",
   "metadata": {},
   "source": [
    "\\begin{bmatrix}\n",
    " \\frac{\\partial (\\frac{x_1}{\\alpha_1}-\\lambda)}{\\partial \\alpha_1} & \\frac{\\partial(\\frac{x_2}{\\alpha_2}-\\lambda)}{\\partial \\alpha_1} & ...\\frac{\\partial(\\frac{x_k}{\\alpha_k}-\\lambda)}{\\partial \\alpha_1} \\\\\n",
    " \\frac{\\partial (\\frac{x_1}{\\alpha_1}-\\lambda)}{\\partial \\alpha_2} & \\frac{\\partial(\\frac{x_2}{\\alpha_2}-\\lambda)}{\\partial \\alpha_2} & ...\\frac{\\partial(\\frac{x_k}{\\alpha_k}-\\lambda)}{\\partial \\alpha_2}\\\\\n",
    "  .& & \\\\\n",
    "  .& & \\\\\n",
    "  .& & \\\\\n",
    " \\frac{\\partial (\\frac{x_1}{\\alpha_1}-\\lambda)}{\\partial \\alpha_k} & \\frac{\\partial(\\frac{x_2}{\\alpha_2}-\\lambda)}{\\partial \\alpha_k} & ...\\frac{\\partial(\\frac{x_k}{\\alpha_k}-\\lambda)}{\\partial \\alpha_k}\\\\\n",
    "\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699ff815",
   "metadata": {},
   "source": [
    "\\begin{bmatrix}\n",
    " \\frac{-x_1}{\\alpha_1^2} & 0 & ... 0 \\\\\n",
    "   0 & \\frac{-x_2}{\\alpha_2^2} & ... 0\\\\\n",
    "  .& & \\\\\n",
    "  .& & \\\\\n",
    "  .& & \\\\\n",
    " 0 & 0 & ... \\frac{-x_k}{\\alpha_k^2}\\\\\n",
    "\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805a34b3",
   "metadata": {},
   "source": [
    "$$As\\; x_1, x_2 ... x_k\\; are\\; all\\; whole\\; numbers,\\; \\geq 0\\; and \\; \\alpha_1, \\alpha_2..., \\alpha_k \\; are \\; probabilities \\; \\geq 0, $$\n",
    "$$\\frac{x_k}{\\alpha_k^2} \\geq 0$$\n",
    "$$\\frac{-x_k}{\\alpha_k^2} \\leq 0$$\n",
    "\n",
    "$$\\to The\\; given\\; matrix\\; is\\; NEGATIVE\\; SEMI\\; DEFINITE$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05485af6",
   "metadata": {},
   "source": [
    "### So the probabilities,\n",
    "## $$\\alpha_k = \\frac{x_k}{\\lambda}$$\n",
    "#### $$where, \\; \\lambda = n\\;.... from \\; (5)$$\n",
    "\n",
    "### are the required estimates for the MLE for the parameters of a k-sided multinomial distribution."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e5e28c5b929416f221fb6605a701fda341e3fadbb2ce7c8964d3325eb15f30d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
