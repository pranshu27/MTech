{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c0660bb",
   "metadata": {},
   "source": [
    "### Q1.a)  Use this function to find minima for (i) x2 + 3x+4 and (ii) x4 – 3x2 +2x. [5 points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "95728efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minima for the function x^2+3x+4 is found at -1.5\n",
      "Minima for the function x^4-3x^2+2x is found at 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "gradient1 = lambda x: 2*x+3 ## specifies gradient for the function x2 + 3x+4\n",
    "gradient2 = lambda x: 4*(x**3)-6*x+2 ## specifies gradient for the function  x4 – 3x2 +2x.\n",
    "\n",
    "def gradient_descent (gradient, init_, learn_rate, n_iter=1000, tol=1e-06):\n",
    "    x = init_\n",
    "    for i in range (n_iter): \n",
    "        delta = (-1)*learn_rate * gradient(x)\n",
    "        if np.all(np.abs (delta) <= tol): \n",
    "            break \n",
    "        x += delta \n",
    "    return  round(x*1000)/1000\n",
    "\n",
    "\n",
    "print('Minima for the function x^2+3x+4 is found at '+ str(gradient_descent(gradient1,4 , 0.1)))\n",
    "\n",
    "print('Minima for the function x^4-3x^2+2x is found at '+ str(gradient_descent(gradient2,4 , 0.01)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241e567d",
   "metadata": {},
   "source": [
    "### Q1.b) Write a gradient function to calculate gradients for a linear regression y = ax + b [10 points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "287c9ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_gradients_calculator(X, Y, learning_rate = 0.01, init_a = 0, init_b = 0, max_iter = 30, tol = 1e-06):\n",
    "    a = 0\n",
    "    b = 0\n",
    "\n",
    "    n = float(len(X)) \n",
    "    \n",
    "    for i in range(max_iter): \n",
    "        Y_pred = a*X + b \n",
    "        \n",
    "        D_a = (-2/n) * sum(X * (Y - Y_pred))  # gradient wrt a\n",
    "        D_b = (-2/n) * sum(Y - Y_pred)  # gradient wrt b\n",
    "        \n",
    "        print('Gradient wrt \"a\": ', str(D_a), 'Gradient wrt \"b\": ', str(D_b))\n",
    "        \n",
    "        delta_a = (-1)*learning_rate * D_a\n",
    "        delta_b = (-1)*learning_rate * D_b\n",
    "\n",
    "        if (np.all(np.abs (delta_a) <= tol) and np.all(np.abs (delta_b) <= tol)): \n",
    "            break\n",
    "        \n",
    "        a+=delta_a\n",
    "        b+=delta_b\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ba958d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient wrt \"a\":  -10.732095840897884 Gradient wrt \"b\":  -4.905313437381427\n",
      "Gradient wrt \"a\":  -8.827398630157248 Gradient wrt \"b\":  -4.495135915979009\n",
      "Gradient wrt \"a\":  -7.247353142900043 Gradient wrt \"b\":  -4.148547328558394\n",
      "Gradient wrt \"a\":  -5.936807080137242 Gradient wrt \"b\":  -3.85483557168645\n",
      "Gradient wrt \"a\":  -4.849974863413731 Gradient wrt \"b\":  -3.6051065198660663\n",
      "Gradient wrt \"a\":  -3.948846887502027 Gradient wrt \"b\":  -3.3919752975366517\n",
      "Gradient wrt \"a\":  -3.201868928685552 Gradient wrt \"b\":  -3.209309981884463\n",
      "Gradient wrt \"a\":  -2.582845828290958 Gradient wrt \"b\":  -3.052018832176958\n",
      "Gradient wrt \"a\":  -2.070031362953744 Gradient wrt \"b\":  -2.9158736535452054\n",
      "Gradient wrt \"a\":  -1.6453726816498069 Gradient wrt \"b\":  -2.7973631585288707\n",
      "Gradient wrt \"a\":  -1.2938830595227675 Gradient wrt \"b\":  -2.6935712318872813\n",
      "Gradient wrt \"a\":  -1.003121176551481 Gradient wrt \"b\":  -2.6020758693756925\n",
      "Gradient wrt \"a\":  -0.7627588300156192 Gradient wrt \"b\":  -2.5208652794442905\n",
      "Gradient wrt \"a\":  -0.5642220621064 Gradient wrt \"b\":  -2.448268233095978\n",
      "Gradient wrt \"a\":  -0.40039323463727633 Gradient wrt \"b\":  -2.3828962421508764\n",
      "Gradient wrt \"a\":  -0.2653637002492161 Gradient wrt \"b\":  -2.3235955571107563\n",
      "Gradient wrt \"a\":  -0.15422847734139356 Gradient wrt \"b\":  -2.2694073169705975\n",
      "Gradient wrt \"a\":  -0.06291579526238811 Gradient wrt \"b\":  -2.219534466540536\n",
      "Gradient wrt \"a\":  0.011954412231539872 Gradient wrt \"b\":  -2.17331429196002\n",
      "Gradient wrt \"a\":  0.07318798151628614 Gradient wrt \"b\":  -2.130195620273269\n",
      "Gradient wrt \"a\":  0.12311404297714455 Gradient wrt \"b\":  -2.089719890974793\n",
      "Gradient wrt \"a\":  0.1636659824315641 Gradient wrt \"b\":  -2.0515054419538172\n",
      "Gradient wrt \"a\":  0.19644865288845523 Gradient wrt \"b\":  -2.01523446394171\n",
      "Gradient wrt \"a\":  0.2227941717406779 Gradient wrt \"b\":  -1.9806421702748864\n",
      "Gradient wrt \"a\":  0.2438082419194544 Gradient wrt \"b\":  -1.947507805750879\n",
      "Gradient wrt \"a\":  0.2604086063203674 Gradient wrt \"b\":  -1.9156471822482433\n",
      "Gradient wrt \"a\":  0.2733569715027069 Gradient wrt \"b\":  -1.8849064818240997\n",
      "Gradient wrt \"a\":  0.28328550977157113 Gradient wrt \"b\":  -1.8551571120369188\n",
      "Gradient wrt \"a\":  0.2907188603928559 Gradient wrt \"b\":  -1.8262914347987136\n",
      "Gradient wrt \"a\":  0.2960923943208595 Gradient wrt \"b\":  -1.798219220408678\n",
      "\n",
      "--- 0.06466221809387207 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "linear_regression_gradients_calculator(X, Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faef964",
   "metadata": {},
   "source": [
    "### Q1.c) Generate artificial data for this regression according to the following protocol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "497a628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np. random.seed (0)\n",
    "X = 2.5*np.random.randn(10000)+1.5 #Arra\n",
    "res=1.5*np.random.randn(10000) # Gene.\n",
    "Y = 2+0.3*X + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "feb4539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_gd(X, Y, learning_rate = 0.01, init_a = 0, init_b = 0, max_iter = 1000, tol = 1e-06):\n",
    "    start_time = time.time()\n",
    "\n",
    "    a = 0\n",
    "    b = 0\n",
    "\n",
    "    n = float(len(X)) \n",
    "    \n",
    "    for i in range(max_iter): \n",
    "        Y_pred = a*X + b \n",
    "        \n",
    "        D_a = (-2/n) * sum(X * (Y - Y_pred))  \n",
    "        D_b = (-2/n) * sum(Y - Y_pred)  \n",
    "        \n",
    "        delta_a = (-1)*learning_rate * D_a\n",
    "        delta_b = (-1)*learning_rate * D_b\n",
    "\n",
    "        if (np.all(np.abs (delta_a) <= tol) and np.all(np.abs (delta_b) <= tol)): \n",
    "            break\n",
    "        \n",
    "        a+=delta_a\n",
    "        b+=delta_b\n",
    "# =============================================================================\n",
    "#         print(i)\n",
    "# =============================================================================\n",
    "    print('It took ', str(i-1), ' iterations to arrive at the desired result')\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    return (round(a,2), round(b,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4c5cfb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took  706  iterations to arrive at the desired result\n",
      "--- 0.9966154098510742 seconds ---\n",
      "\n",
      "Using batch gradient descent with a learning rate of 0.01, the values of (a,b) are  (0.3, 2.02)\n"
     ]
    }
   ],
   "source": [
    " print('\\nUsing batch gradient descent with a learning rate of 0.01, the values of (a,b) are ', str(lr_gd(X, Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f861c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e3c23fe",
   "metadata": {},
   "source": [
    "### Q1.d) Implement minibatch stochastic gradient descent using the code base you have developed so far.[15 points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5066f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batches(ins, outs, bs): #a utility function to create minibatches of a specific size\n",
    "    for start_idx in range(0, ins.shape[0], bs):\n",
    "        end_idx = min(start_idx + bs, ins.shape[0])\n",
    "        ids = slice(start_idx, end_idx)\n",
    "        yield ins[ids], outs[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7c7ba2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit   \n",
    "def lr_mb_gd(X, Y, batch_size, learning_rate = 0.01, init_a = 0, init_b = 0, max_iter = 1000, tol = 1e-06):\n",
    "    start_time = time.time()\n",
    "    a = 0\n",
    "    b = 0\n",
    "\n",
    "    n = float(len(X)) \n",
    "    \n",
    "    for i in range(max_iter): \n",
    "        for mini_batch in  create_mini_batches(X, Y, batch_size):\n",
    "            \n",
    "            X_mini, Y_mini = mini_batch\n",
    "            Y_pred = a*X_mini + b \n",
    "            \n",
    "            D_a = (-2/n) * sum(X_mini * (Y_mini - Y_pred))  \n",
    "            D_b = (-2/n) * sum(Y_mini - Y_pred)  \n",
    "            \n",
    "            delta_a = (-1)*learning_rate * D_a\n",
    "            delta_b = (-1)*learning_rate * D_b\n",
    "    \n",
    "            if (np.all(np.abs (delta_a) <= tol) and np.all(np.abs (delta_b) <= tol)): \n",
    "                break\n",
    "            \n",
    "            a+=delta_a\n",
    "            b+=delta_b\n",
    "# =============================================================================\n",
    "#         print(i)\n",
    "# =============================================================================\n",
    "    print('It took ', str(i-1), ' iterations to arrive at the desired result')\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    return (round(a,2), round(b,2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0cdf80",
   "metadata": {},
   "source": [
    "> I will try various batch sizes(with a fixed learning rate __0.01__) and will finally pick a size that gives us the best values of _a_ and _b_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "04984717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took  998  iterations to arrive at the desired result\n",
      "--- 2.2248096466064453 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4, 1.37)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_mb_gd(X, Y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5a648793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took  998  iterations to arrive at the desired result\n",
      "--- 2.7921712398529053 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.32, 1.79)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_mb_gd(X, Y, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7df8c110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took  998  iterations to arrive at the desired result\n",
      "--- 4.128742218017578 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.29, 1.96)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_mb_gd(X, Y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2e1eb4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took  998  iterations to arrive at the desired result\n",
      "--- 4.336106538772583 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3, 2.0)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_mb_gd(X, Y, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb32c845",
   "metadata": {},
   "source": [
    "> As observed, I will choose __15__ as my batch size, as this gives precise values of _a_ and _b_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c11a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c23c3ac",
   "metadata": {},
   "source": [
    "### Q1.e) Does SGD do better or worse in terms of time performance on our data? Is there an optimal minibatch size that works best? Quantify and interpret your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025dd72f",
   "metadata": {},
   "source": [
    "> We tried different batch sizes for Stochastic Mini Batch GD and got the most optinum results for a size of __15__\n",
    "\n",
    "### Batch gradient descent clearly seems to be a better option for doing linear regression\n",
    "1. It took __998__ iterations even with the most optimum batch size in Mini Batch Stochastic gradient descent where as it took __706__ in batch versison\n",
    "2. It took __~1s__ execution time for Batch GD whereas it took __~4.34s__ for the other one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc43484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270c097a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
