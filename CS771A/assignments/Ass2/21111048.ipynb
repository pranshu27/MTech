{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Q1.a)  Use this function to find minima for (i) x2 + 3x+4 and (ii) x4 – 3x2 +2x. [5 points]\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "source": [
    "\n",
    "\n",
    "gradient1 = lambda x: 2*x+3 ## specifies gradient for the function x2 + 3x+4\n",
    "gradient2 = lambda x: 4*(x**3)-6*x+2 ## specifies gradient for the function  x4 – 3x2 +2x.\n",
    "\n",
    "def gradient_descent (gradient, init_, learn_rate, n_iter=1000, tol=1e-06):\n",
    "    x = init_\n",
    "    for i in range (n_iter): \n",
    "        delta = (-1)*learn_rate * gradient(x)\n",
    "        if np.all(np.abs (delta) <= tol): \n",
    "            break \n",
    "        x += delta \n",
    "    print('Iterations:', str(i+1))\n",
    "    return  round(x*1000)/1000\n",
    "\n",
    "x1 = gradient_descent(gradient1,4 , 0.1)\n",
    "min_func1 = x1**2+3*x1+4\n",
    "print('Minima for the function x^2+3x+4 is found at '+ str(x1), 'and the value is ', str(min_func1))\n",
    "\n",
    "x2 = gradient_descent(gradient2,-1 , 0.01)\n",
    "min_func2 = x2**4 - 3*(x2**2) + 2*x2\n",
    "print('Minima for the function x^4-3x^2+2x is found at '+ str(x2), 'and the value is ', str(min_func2))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iterations: 64\n",
      "Minima for the function x^2+3x+4 is found at -1.5 and the value is  1.75\n",
      "Iterations: 66\n",
      "Minima for the function x^4-3x^2+2x is found at -1.366 and the value is  -4.848076206064\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q1.b) Write a gradient function to calculate gradients for a linear regression y = ax + b [10 points]\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "source": [
    "def linear_regression_gradients_calculator(X, Y, learning_rate = 0.01, init_a = 0, init_b = 0, max_iter = 30, tol = 1e-06):\n",
    "    a = 0\n",
    "    b = 0\n",
    "\n",
    "    n = float(len(X)) \n",
    "    \n",
    "    for i in range(max_iter): \n",
    "        Y_pred = a*X + b \n",
    "        \n",
    "        D_a = (-2/n) * sum(X * (Y - Y_pred))  # gradient wrt a\n",
    "        D_b = (-2/n) * sum(Y - Y_pred)  # gradient wrt b\n",
    "        \n",
    "        print('Gradient wrt \"a\": ', str(D_a), 'Gradient wrt \"b\": ', str(D_b))\n",
    "        \n",
    "        delta_a = (-1)*learning_rate * D_a\n",
    "        delta_b = (-1)*learning_rate * D_b\n",
    "\n",
    "        if (np.all(np.abs (delta_a) <= tol) and np.all(np.abs (delta_b) <= tol)): \n",
    "            break\n",
    "        \n",
    "        a+=delta_a\n",
    "        b+=delta_b\n",
    "        \n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "source": [
    "np. random.seed (0)\n",
    "X = 2.5*np.random.randn(10000)+1.5 #Arra\n",
    "res=1.5*np.random.randn(10000) # Gene.\n",
    "Y = 2+0.3*X + res"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "source": [
    "linear_regression_gradients_calculator(X, Y)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gradient wrt \"a\":  -10.732095840897884 Gradient wrt \"b\":  -4.905313437381427\n",
      "Gradient wrt \"a\":  -8.827398630157248 Gradient wrt \"b\":  -4.495135915979009\n",
      "Gradient wrt \"a\":  -7.247353142900043 Gradient wrt \"b\":  -4.148547328558394\n",
      "Gradient wrt \"a\":  -5.936807080137242 Gradient wrt \"b\":  -3.85483557168645\n",
      "Gradient wrt \"a\":  -4.849974863413731 Gradient wrt \"b\":  -3.6051065198660663\n",
      "Gradient wrt \"a\":  -3.948846887502027 Gradient wrt \"b\":  -3.3919752975366517\n",
      "Gradient wrt \"a\":  -3.201868928685552 Gradient wrt \"b\":  -3.209309981884463\n",
      "Gradient wrt \"a\":  -2.582845828290958 Gradient wrt \"b\":  -3.052018832176958\n",
      "Gradient wrt \"a\":  -2.070031362953744 Gradient wrt \"b\":  -2.9158736535452054\n",
      "Gradient wrt \"a\":  -1.6453726816498069 Gradient wrt \"b\":  -2.7973631585288707\n",
      "Gradient wrt \"a\":  -1.2938830595227675 Gradient wrt \"b\":  -2.6935712318872813\n",
      "Gradient wrt \"a\":  -1.003121176551481 Gradient wrt \"b\":  -2.6020758693756925\n",
      "Gradient wrt \"a\":  -0.7627588300156192 Gradient wrt \"b\":  -2.5208652794442905\n",
      "Gradient wrt \"a\":  -0.5642220621064 Gradient wrt \"b\":  -2.448268233095978\n",
      "Gradient wrt \"a\":  -0.40039323463727633 Gradient wrt \"b\":  -2.3828962421508764\n",
      "Gradient wrt \"a\":  -0.2653637002492161 Gradient wrt \"b\":  -2.3235955571107563\n",
      "Gradient wrt \"a\":  -0.15422847734139356 Gradient wrt \"b\":  -2.2694073169705975\n",
      "Gradient wrt \"a\":  -0.06291579526238811 Gradient wrt \"b\":  -2.219534466540536\n",
      "Gradient wrt \"a\":  0.011954412231539872 Gradient wrt \"b\":  -2.17331429196002\n",
      "Gradient wrt \"a\":  0.07318798151628614 Gradient wrt \"b\":  -2.130195620273269\n",
      "Gradient wrt \"a\":  0.12311404297714455 Gradient wrt \"b\":  -2.089719890974793\n",
      "Gradient wrt \"a\":  0.1636659824315641 Gradient wrt \"b\":  -2.0515054419538172\n",
      "Gradient wrt \"a\":  0.19644865288845523 Gradient wrt \"b\":  -2.01523446394171\n",
      "Gradient wrt \"a\":  0.2227941717406779 Gradient wrt \"b\":  -1.9806421702748864\n",
      "Gradient wrt \"a\":  0.2438082419194544 Gradient wrt \"b\":  -1.947507805750879\n",
      "Gradient wrt \"a\":  0.2604086063203674 Gradient wrt \"b\":  -1.9156471822482433\n",
      "Gradient wrt \"a\":  0.2733569715027069 Gradient wrt \"b\":  -1.8849064818240997\n",
      "Gradient wrt \"a\":  0.28328550977157113 Gradient wrt \"b\":  -1.8551571120369188\n",
      "Gradient wrt \"a\":  0.2907188603928559 Gradient wrt \"b\":  -1.8262914347987136\n",
      "Gradient wrt \"a\":  0.2960923943208595 Gradient wrt \"b\":  -1.798219220408678\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q1.c) Generate artificial data for this regression according to the following protocol\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "source": [
    "batch_time = 0\n",
    "def lr_gd(X, Y, learning_rate = 0.05, init_a = 0, init_b = 0, max_iter = 1000, tol = 1e-06):\n",
    "    start_time = time.time()\n",
    "\n",
    "    a = 0\n",
    "    b = 0\n",
    "\n",
    "    n = float(len(X)) \n",
    "    \n",
    "    for i in range(max_iter): \n",
    "        Y_pred = a*X + b \n",
    "        \n",
    "        D_a = (-2/n) * sum(X * (Y - Y_pred))  \n",
    "        D_b = (-2/n) * sum(Y - Y_pred)  \n",
    "        \n",
    "        delta_a = (-1)*learning_rate * D_a\n",
    "        delta_b = (-1)*learning_rate * D_b\n",
    "\n",
    "        if (np.all(np.abs (delta_a) <= tol) and np.all(np.abs (delta_b) <= tol)): \n",
    "            break\n",
    "        \n",
    "        a+=delta_a\n",
    "        b+=delta_b\n",
    "# =============================================================================\n",
    "#         print(i)\n",
    "# =============================================================================\n",
    "    #print('It took ', str(i+1), ' iterations to arrive at the desired result')\n",
    "    #batch_time = time.time() - start_time\n",
    "    #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    return round(a,2), round(b,2),time.time() - start_time \n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "source": [
    " print('\\nUsing batch gradient descent with a learning rate of 0.01, the values of (a,b, execution time) are ', str(lr_gd(X, Y)))\n",
    "\n",
    " a,b,batch_time = lr_gd(X, Y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Using batch gradient descent with a learning rate of 0.01, the values of (a,b, execution time) are  (0.3, 2.02, 0.30642127990722656)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q1.d) Implement minibatch stochastic gradient descent using the code base you have developed so far.[15 points]\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "source": [
    "def lr_mb_gd(X, Y, batch_size, learning_rate = 0.05, init_a = 0, init_b = 0, max_iter = 1000, tol = 1e-06):\n",
    "    start_time = time.time()\n",
    "    a = 0\n",
    "    b = 0\n",
    "\n",
    "    n = float(len(X)) \n",
    "    \n",
    "    for i in range(max_iter): \n",
    "        random_index=random.randint(0,n-batch_size-1)\n",
    "        X_mini= X[random_index:random_index+batch_size]\n",
    "        Y_mini = Y[random_index:random_index+batch_size]\n",
    "        \n",
    "        \n",
    "        Y_pred = a*X_mini + b \n",
    "\n",
    "        D_a = (-2/n) * sum(X_mini * (Y_mini - Y_pred))  \n",
    "        D_b = (-2/n) * sum(Y_mini - Y_pred)  \n",
    "\n",
    "        delta_a = (-1)*learning_rate * D_a\n",
    "        delta_b = (-1)*learning_rate * D_b\n",
    "\n",
    "        if (np.all(np.abs (delta_a) <= tol) and np.all(np.abs (delta_b) <= tol)): \n",
    "            break\n",
    "\n",
    "        a+=delta_a\n",
    "        b+=delta_b\n",
    "# =============================================================================\n",
    "#         print(i)\n",
    "# =============================================================================\n",
    "    #print('It took ', str(i+1), ' iterations to arrive at the desired result')\n",
    "    #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    #print (round(a,2), round(b,2))\n",
    "    return str(i+1), time.time() - start_time, round(a,2), round(b,2)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## I will try various batch sizes(with a fixed learning rate __0.05__) and will finally pick a size that gives us the best values of _a_ and _b_"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "source": [
    "batch_sizes = [i for i in range(1,1000)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "source": [
    "out = []\n",
    "for b in batch_sizes:\n",
    "    #print('batch_size ', str(b))\n",
    "    tmp = {}\n",
    "    tmp['batch_size'] = b\n",
    "    tmp['iterations'], tmp['exec_time'], tmp['a'], tmp['b'] = lr_mb_gd(X, Y, b)\n",
    "    #print('\\n')\n",
    "    out.append(tmp)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "source": [
    "df = pd.DataFrame(out)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "source": [
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>iterations</th>\n",
       "      <th>exec_time</th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>510</td>\n",
       "      <td>0.011537</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.025644</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.022442</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.020232</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_size iterations  exec_time     a     b\n",
       "0           1        101   0.002072  0.01  0.00\n",
       "1           2        510   0.011537  0.05  0.02\n",
       "2           3       1000   0.025644  0.14  0.07\n",
       "3           4       1000   0.022442  0.18  0.09\n",
       "4           5       1000   0.020232  0.22  0.11"
      ]
     },
     "metadata": {},
     "execution_count": 200
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## As observed, we are getting the most optimal values for a and b using a batch size and time taken(seconds) as under:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "source": [
    "min_time = df[(df['a']==0.30) & (df['b']==2.0)]['exec_time'].min()\n",
    "min_time\n",
    "\n",
    "batch_size = list(df[df['exec_time'] == min_time]['batch_size'])[0]\n",
    "batch_size\n",
    "\n",
    "print(\"Batch Size: \", str(batch_size), \" Execution Time: \", str(min_time))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batch Size:  657  Execution Time:  0.1260068416595459\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The percentage reduction in execution time is as below:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "source": [
    "print('Time_reduction_perc =',round(((batch_time - min_time)/batch_time)*100,2), '%')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time_reduction_perc = 53.52 %\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q1.e) Does SGD do better or worse in terms of time performance on our data? Is there an optimal minibatch size that works best? Quantify and interpret your findings."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference: \n",
    "## With fixed values for parameters like **learn rate = 0.05, and max_iter = 1000**, we can clearly observe a considerable **reduction** in execution time as calculated above when we use Stochastic Mini Batch GD. \n",
    "## However, it took __1000__ iterations even with the most optimum batch size in Mini Batch Stochastic gradient descent where as it took __706__ in batch version. Therefore, in terms of using CPU, Batch seems to be a better option than Mini Batch.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q2. Surprise! This problem too builds on a problem that I asked in the mid-sem exam. Consider again this Bayesian network and calculate:\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (i) the probability that someone has both cold and a fever [5 points]\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$ P(cold \\cap fever)  = P(fever / cold) * P(cold) \n",
    "                       = 0.307*0.02\n",
    "                       = 0.00614 --------ANS.\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (ii) the probability that someone who has a cough has a cold. [10 points]\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$ P(cold/cough) = P(cough \\cap cold)/P(cold) ------- (0)$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$P(lungDisease) = P(lungDisease/smokes)*P(smokes) + P(lungDisease/not smokes) * P(not smokes)$$\n",
    "$$0.2*0.1009 + 0.8*0.001 = 0.02098 -------(1)$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$P(cold) = P(lungDisease)*P(cold)*P(cough/lungDisease \\cap cold) + P(lungDisease)*P(\\overline{cold})*P(cough/lungDisease \\cap\\overline{cold}) + P(\\overline{lungDisease})*P(cold)*P(cough/\\overline{lungDisease} \\cap cold) + P(\\overline{lungDisease})*P(\\overline{cold})*P(cough/\\overline{lungDisease} \\cap \\overline{cold}) $$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$= 0.02098*0.02*0.7525 + 0.02098*0.98*0.505 + 0.97902*0.02*0.01 +  0.97902*0.98*0.505= 0.03 -------------(2)$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$P(cough \\cap cold) = P(lungDisease)*P(cold)*P(cough/lungDisease \\cap cold) +  P(\\overline{lungDisease})*P(cold)*P(cough/\\overline{lungDisease} \\cap cold)$$\n",
    "$$ = 0.02098*0.02*0.7525 + 0.97902*0.02*0.01 = 0.0101 ------------(3)$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Putting  (2) and (3) in (0), we get, \n",
    "$$0.0101/0.03 = 0.3367------------ANS.$$ "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "interpreter": {
   "hash": "9e5e28c5b929416f221fb6605a701fda341e3fadbb2ce7c8964d3325eb15f30d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}