{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Q1.a)  Use this function to find minima for (i) x2 + 3x+4 and (ii) x4 – 3x2 +2x. [5 points]\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "source": [
    "gradient1 = lambda x: 2*x+3 ## specifies gradient for the function x2 + 3x+4\n",
    "gradient2 = lambda x: 4*(x**3)-6*x+2 ## specifies gradient for the function  x4 – 3x2 +2x.\n",
    "\n",
    "def gradient_descent (gradient, init_, learn_rate, n_iter=1000, tol=1e-06):\n",
    "    x = init_\n",
    "    for i in range (n_iter): \n",
    "        delta = (-1)*learn_rate * gradient(x)\n",
    "        if np.all(np.abs (delta) <= tol): \n",
    "            break \n",
    "        x += delta \n",
    "    print('Iterations:', str(i+1))\n",
    "    return  round(x*1000)/1000\n",
    "\n",
    "x1 = gradient_descent(gradient1,4 , 0.1)\n",
    "min_func1 = x1**2+3*x1+4\n",
    "print('Minima for the function x^2+3x+4 is found at '+ str(x1), 'and the minimum value of the function is', str(min_func1))\n",
    "\n",
    "x2 = gradient_descent(gradient2,-1 , 0.01)\n",
    "min_func2 = x2**4 - 3*(x2**2) + 2*x2\n",
    "print('Minima for the function x^4-3x^2+2x is found at '+ str(x2), 'and the minimum value of the function is', str(min_func2))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iterations: 64\n",
      "Minima for the function x^2+3x+4 is found at -1.5 and the minimum value of the function is 1.75\n",
      "Iterations: 66\n",
      "Minima for the function x^4-3x^2+2x is found at -1.366 and the minimum value of the function is -4.848076206064\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q1.b) Write a gradient function to calculate gradients for a linear regression y = ax + b [10 points]\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The gradients w.r.t __a__ and __b__ are calculated as: \n",
    "$$\\nabla_a = \\frac{-2}{n} * \\sum(X * (Y - Y_{pred}))$$\n",
    "$$\\nabla_b = \\frac{-2}{n} * \\sum(Y - Y_pred)$$\n",
    "<br>\n",
    "\n",
    "$$where, \\; Y_{pred} = a\\ast X + b$$\n",
    "\n",
    "### The same are used in the function below to elaborate their actual usage:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "source": [
    "def linear_regression_gradients_calculator(X, Y, learning_rate = 0.01, init_a = 0, init_b = 0, max_iter = 10, tol = 1e-06):\n",
    "    a = 0\n",
    "    b = 0\n",
    "\n",
    "    n = len(X)\n",
    "    \n",
    "    for i in range(max_iter): \n",
    "        Y_pred = a*X + b \n",
    "        \n",
    "        D_a = (-2/n) * sum(X * (Y - Y_pred))  # gradient wrt a\n",
    "        D_b = (-2/n) * sum(Y - Y_pred)  # gradient wrt b\n",
    "        \n",
    "        print('Gradient wrt \"a\": ', str(D_a), 'Gradient wrt \"b\": ', str(D_b))\n",
    "        \n",
    "        delta_a = (-1)*learning_rate * D_a\n",
    "        delta_b = (-1)*learning_rate * D_b\n",
    "\n",
    "        if (np.all(np.abs (delta_a) <= tol) and np.all(np.abs (delta_b) <= tol)): \n",
    "            break\n",
    "        \n",
    "        a+=delta_a\n",
    "        b+=delta_b\n",
    "        \n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Here we are trying to generate some artificial data: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "source": [
    "np.random.seed (5)\n",
    "X = 2.5*np.random.randn(10000)+1.5 #Ar\n",
    "res=1.5*np.random.randn(10000) # Gene.\n",
    "Y = 2+0.3*X + res"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### We are just trying to run the template function built above, for 10 iterations in order to show-case the way gradients will look after each iteration. Take a look:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "source": [
    "linear_regression_gradients_calculator(X, Y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gradient wrt \"a\":  -11.166205252170924 Gradient wrt \"b\":  -4.9101305555695545\n",
      "Gradient wrt \"a\":  -9.118835828208706 Gradient wrt \"b\":  -4.477105542253759\n",
      "Gradient wrt \"a\":  -7.432848792885496 Gradient wrt \"b\":  -4.114132086707702\n",
      "Gradient wrt \"a\":  -6.044647691634381 Gradient wrt \"b\":  -3.8089729858853363\n",
      "Gradient wrt \"a\":  -4.901825126160185 Gradient wrt \"b\":  -3.551542741779738\n",
      "Gradient wrt \"a\":  -3.961194208801586 Gradient wrt \"b\":  -3.3335290197245313\n",
      "Gradient wrt \"a\":  -3.1871663522486777 Gradient wrt \"b\":  -3.148080705029388\n",
      "Gradient wrt \"a\":  -2.550414462239225 Gradient wrt \"b\":  -2.989550841000976\n",
      "Gradient wrt \"a\":  -2.0267713209718705 Gradient wrt \"b\":  -2.8532847928180876\n",
      "Gradient wrt \"a\":  -1.596321783048875 Gradient wrt \"b\":  -2.7354456804713787\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q1.c) Generate artificial data for this regression according to the following protocol\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1027,
   "source": [
    "batch_time = 0\n",
    "def lr_gd(X, Y, learning_rate = 0.01, init_a = 0, init_b = 0, max_iter = 10000, tol = 1e-04):\n",
    "    start_time = time.time()\n",
    "\n",
    "    a = 0\n",
    "    b = 0\n",
    "\n",
    "    n = float(len(X)) \n",
    "    \n",
    "    for i in range(max_iter): \n",
    "        Y_pred = a*X + b \n",
    "        \n",
    "        D_a = (-2/n) * np.dot(X.T,(Y - Y_pred))  \n",
    "        D_b = (-2/n) * sum(Y - Y_pred)  \n",
    "        \n",
    "        delta_a = (-1)*learning_rate * D_a\n",
    "        delta_b = (-1)*learning_rate * D_b\n",
    "\n",
    "        if (np.all(np.abs (delta_a) <= tol) & np.all(np.abs (delta_b) <= tol)): \n",
    "            break\n",
    "        \n",
    "        a+=delta_a\n",
    "        b+=delta_b\n",
    "# =============================================================================\n",
    "#         print(i)\n",
    "# =============================================================================\n",
    "    #print('It took ', str(i+1), ' iterations to arrive at the desired result')\n",
    "    #batch_time = time.time() - start_time\n",
    "    #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    return round(a,2), round(b,2),time.time() - start_time, i+1 \n",
    "print('\\nUsing batch gradient descent with a learning rate of 0.01, the values of (a,b, execution time, #iterations) are ', str(lr_gd(X, Y)))\n",
    "\n",
    "a,b,batch_time, iterations = lr_gd(X, Y)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Using batch gradient descent with a learning rate of 0.01, the values of (a,b, execution time, #iterations) are  (0.31, 1.99, 0.46634364128112793, 391)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Here we are trying to calculate partial derivatives in each iteration, w.r.t to the free parameteres __a__ and __b__. We updated their values based on the learning rate and the gradients calculated, and break the loop under following two scenarios:\n",
    "### 1. Iterations have equalled to max_iter\n",
    "### 2. Delta_a and delta_b have dropped below a specified error\n",
    "$$10^{-6}, in \\; this \\; case$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q1.d) Implement minibatch stochastic gradient descent using the code base you have developed so far.[15 points]\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "source": [
    "def lr_mb_gd(X, Y, batch_size, learning_rate = 0.01, init_a = 0, init_b = 0, max_iter = 10000, tol = 1e-04):\n",
    "    start_time = time.time()\n",
    "    a = 0\n",
    "    b = 0\n",
    "\n",
    "    n = float(len(X)) \n",
    "    \n",
    "    for i in range(max_iter): \n",
    "        #np.random.seed (5)\n",
    "        random_index=np.random.randint(0,n-batch_size-1)\n",
    "        X_mini= X[random_index:random_index+batch_size]\n",
    "        Y_mini = Y[random_index:random_index+batch_size]\n",
    "        \n",
    "        n_tmp = len(X_mini)\n",
    "        Y_pred = a*X_mini + b \n",
    "\n",
    "        D_a = (-2/n_tmp) * np.dot(X_mini.T,(Y_mini - Y_pred))  \n",
    "        D_b = (-2/n_tmp) * sum(Y_mini - Y_pred)  \n",
    "\n",
    "        delta_a = (-1)*learning_rate * D_a\n",
    "        delta_b = (-1)*learning_rate * D_b\n",
    "\n",
    "        if (np.all(np.abs (delta_a) <= tol) & np.all(np.abs (delta_b) <= tol)): \n",
    "            break\n",
    "\n",
    "        a+=delta_a\n",
    "        b+=delta_b\n",
    "\n",
    "# =============================================================================\n",
    "#         print(i)\n",
    "# =============================================================================\n",
    "    #print('It took ', str(i+1), ' iterations to arrive at the desired result')\n",
    "    #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    #print (round(a,2), round(b,2))\n",
    "    return str(i+1), time.time() - start_time, round(a,2), round(b,2)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## I will try various batch sizes(with a fixed learning rate __0.05__) and will finally pick a size that gives us the best values of _a_ and _b_ in minimal time"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "source": [
    "batch_sizes = [i for i in range(1,1000)]\n",
    "out = []\n",
    "for b in batch_sizes:\n",
    "    #print('batch_size ', str(b))\n",
    "    tmp = {}\n",
    "    tmp['batch_size'] = b\n",
    "    tmp['iterations'], tmp['exec_time'], tmp['a'], tmp['b'] = lr_mb_gd(X, Y, b)\n",
    "    #print('\\n')\n",
    "    out.append(tmp)\n",
    "df = pd.DataFrame(out)\n",
    "df.head(20)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>iterations</th>\n",
       "      <th>exec_time</th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>0.004830</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.363582</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.397294</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.317170</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.317594</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.352479</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>938</td>\n",
       "      <td>0.032967</td>\n",
       "      <td>0.31</td>\n",
       "      <td>2.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2835</td>\n",
       "      <td>0.087462</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.289855</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.289151</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.297439</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.290268</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.301395</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>5400</td>\n",
       "      <td>0.158878</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>2451</td>\n",
       "      <td>0.073176</td>\n",
       "      <td>0.34</td>\n",
       "      <td>2.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>2375</td>\n",
       "      <td>0.072140</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.312096</td>\n",
       "      <td>0.26</td>\n",
       "      <td>2.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.299833</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>3883</td>\n",
       "      <td>0.115843</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>6769</td>\n",
       "      <td>0.201010</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    batch_size iterations  exec_time     a     b\n",
       "0            1         89   0.004830  0.21  1.52\n",
       "1            2      10000   0.363582  0.23  1.83\n",
       "2            3      10000   0.397294  0.25  1.98\n",
       "3            4      10000   0.317170  0.27  1.99\n",
       "4            5      10000   0.317594  0.14  1.93\n",
       "5            6      10000   0.352479  0.22  2.07\n",
       "6            7        938   0.032967  0.31  2.08\n",
       "7            8       2835   0.087462  0.30  2.00\n",
       "8            9      10000   0.289855  0.36  1.99\n",
       "9           10      10000   0.289151  0.36  1.96\n",
       "10          11      10000   0.297439  0.34  1.96\n",
       "11          12      10000   0.290268  0.31  1.99\n",
       "12          13      10000   0.301395  0.29  2.05\n",
       "13          14       5400   0.158878  0.30  2.07\n",
       "14          15       2451   0.073176  0.34  2.01\n",
       "15          16       2375   0.072140  0.21  1.98\n",
       "16          17      10000   0.312096  0.26  2.02\n",
       "17          18      10000   0.299833  0.30  2.08\n",
       "18          19       3883   0.115843  0.24  2.00\n",
       "19          20       6769   0.201010  0.29  1.99"
      ]
     },
     "metadata": {},
     "execution_count": 1029
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## As observed, we are getting the most optimal values for a and b using a batch size and time taken(seconds) as under:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "source": [
    "min_time = df[(df['a'].isin([0.30,0.31,0.29])) & (df['b'].isin([2.00,2.01,2.02,1.99,1.98]))]['exec_time'].min()\n",
    "min_time\n",
    "\n",
    "batch_size = list(df[df['exec_time'] == min_time]['batch_size'])[0]\n",
    "batch_size\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "349"
      ]
     },
     "metadata": {},
     "execution_count": 1030
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The percentage reduction in execution time is as below:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "source": [
    "\n",
    "print(\"Batch Size: \", str(batch_size), \" Execution Time: \", str(min_time))\n",
    "print('Time_reduction_perc =',round(((batch_time - min_time)/batch_time)*100,2), '%')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batch Size:  349  Execution Time:  0.018146753311157227\n",
      "Time_reduction_perc = 94.8 %\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q1.e) Does SGD do better or worse in terms of time performance on our data? Is there an optimal minibatch size that works best? Quantify and interpret your findings."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference: \n",
    "## With fixed values for parameters like **learn rate = 0.05, and max_iter = 1000**, we can clearly observe a considerable **reduction** in execution time as calculated above when we use Stochastic Mini Batch GD. \n",
    "## However, it took __1000__ iterations even with the most optimum batch size in Mini Batch Stochastic gradient descent where as it took __706__ in batch version. Therefore, in terms of using CPU, Batch seems to be a better option than Mini Batch.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q2. Surprise! This problem too builds on a problem that I asked in the mid-sem exam. Consider again this Bayesian network and calculate:\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (i) the probability that someone has both cold and a fever [5 points]\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$ P(cold \\cap fever)  = P(fever / cold) * P(cold) \n",
    "                       = 0.307*0.02\n",
    "                       = 0.00614 --------ANS.\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (ii) the probability that someone who has a cough has a cold. [10 points]\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$ P(cold/cough) = P(cough \\cap cold)/P(cough) ------- (0)$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$P(lungDisease) = P(lungDisease/smokes)*P(smokes) + P(lungDisease/not smokes) * P(not smokes)$$\n",
    "$$0.2*0.1009 + 0.8*0.001 = 0.02098 -------(1)$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$P(cold) = P(lungDisease)*P(cold)*P(cough/lungDisease \\cap cold) + P(lungDisease)*P(\\overline{cold})*P(cough/lungDisease \\cap\\overline{cold}) + P(\\overline{lungDisease})*P(cold)*P(cough/\\overline{lungDisease} \\cap cold) + P(\\overline{lungDisease})*P(\\overline{cold})*P(cough/\\overline{lungDisease} \\cap \\overline{cold}) $$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$= 0.02098*0.02*0.7525 + 0.02098*0.98*0.505 + 0.97902*0.02*0.01 +  0.97902*0.98*0.505= 0.03 -------------(2)$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$P(cough \\cap cold) = P(lungDisease)*P(cold)*P(cough/lungDisease \\cap cold) +  P(\\overline{lungDisease})*P(cold)*P(cough/\\overline{lungDisease} \\cap cold)$$\n",
    "$$ = 0.02098*0.02*0.7525 + 0.97902*0.02*0.01 = 0.0101 ------------(3)$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Putting  (2) and (3) in (0), we get, \n",
    "$$0.0101/0.03 = 0.3367------------ANS.$$ "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q3. Derive the MLE for the parameters of a k-sided multinomial distribution. [10 points]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$P(X_1=x_1, X_2=x_2 .... X_k=x_k) = \\frac{n_1!}{{x_1!}{x_2!}....{x_k!}}{{p_1^{x_1}}{p_2^{x_2}}...{p_k^{x_k}}} ............ (1)$$\n",
    "$$\\sum{x_i}=n, \\sum{p_i}=n$$\n",
    "$$L(x_1, x_2, ..., x_k; p_1, p_2...., p_k) = L(p) = \\frac{n_1!}{{x_1!}{x_2!}....{x_k!}}{{p_1^{x_1}}{p_2^{x_2}}...{p_k^{x_k}}}$$\n",
    "$$l(p) = \\log(L(p))$$\n",
    "$$\\log(\\frac{n_1!}{{x_1!}{x_2!}....{x_k!}}{{p_1^{x_1}}{p_2^{x_2}}...{p_k^{x_k}}})$$\n",
    "$$\\log(n!)+ \\sum_{i=1}^k{x_i\\log(p_i)} - \\sum_{i=1}^k\\log(x_i!)$$\n",
    "\n",
    "With an addition of Lagrange multiplier:\n",
    "\n",
    "$$l(p, \\lambda) = L(p) + \\lambda(1-\\sum_{i=1}^kp_i) ........... (2)$$\n",
    "\n",
    "The task is to find the minima of the above function:\n",
    "\n",
    "$$\\frac{\\partial l(p, \\lambda)}{\\partial p_1} = \\frac{\\partial L(p)}{\\partial p_1} + \\frac{\\partial \\lambda(1-\\sum_{i=1}^kp_i)}{\\partial p_1}$$\n",
    "$$ = \\frac{\\partial \\log(n!)+ \\sum_{i=1}^k{x_i\\log(p_i)} - \\sum_{i=1}^k\\log(x_i!)}{\\partial p_1} + \\frac{\\partial \\lambda(1-\\sum_{i=1}^kp_i)}{\\partial p_1}$$ \n",
    "$$= 0+\\frac{p_1}{x_1} -0+0-\\lambda = \\frac{p_1}{x_1}-\\lambda$$\n",
    "<br>\n",
    "Similarly, we can show that, \n",
    "\n",
    "$$\\frac{\\partial l(p, \\lambda)}{\\partial p_k} = \\frac{p_k}{x_k}-\\lambda $$\n",
    "\n",
    "<br>\n",
    "In order to find the maxima, we need to equate these gradients to 0:\n",
    "\n",
    "$$\\frac{x_1}{p_1} = \\lambda$$\n",
    "i.e.\n",
    "$$p_1 = \\frac{x_1}{\\lambda}$$\n",
    "$$p_2 = \\frac{x_2}{\\lambda}$$\n",
    ".\n",
    ".\n",
    ".\n",
    "$$p_k = \\frac{x_k}{\\lambda} ............ (4)$$\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\\to p_1+p_2...p_k = \\frac{x_1}{\\lambda} + \\frac{x_2}{\\lambda} .... + \\frac{x_k}{\\lambda}....(3)$$\n",
    "<br>\n",
    "\n",
    "$$Since,\\;\\; \\sum(p_i) = 1, (3)\\; now\\; becomes, $$\n",
    "\n",
    "$$\\to 1 = \\frac{1}{\\lambda}(x_1+x_2+ .... x_k)$$\n",
    "$$\\to 1 = \\frac{\\sum(x_i)}{\\lambda}$$\n",
    "$$\\to\\lambda = n -----------(5)$$\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Differentiating again in order to get the maxima, we will obtain a Hessian matrix of the form:\n",
    "$$\\frac{\\partial^2l(p, \\lambda)}{\\partial p^2} = \\begin{bmatrix}\n",
    " \\frac{\\partial^2l(p, \\lambda)}{\\partial p_1^2} & \\frac{\\partial^2l(p, \\lambda)}{\\partial p_1\\partial p_2} & ...\\frac{\\partial^2l(p, \\lambda)}{\\partial p_1\\partial p_k} \\\\\n",
    " \n",
    " \\frac{\\partial^2l(p, \\lambda)}{\\partial p_2\\partial p_1} & \\frac{\\partial^2l(p, \\lambda)}{\\partial p_2^2} & ...\\frac{\\partial^2l(p, \\lambda)}{\\partial p_2\\partial p_k}\\\\\n",
    "  .& & \\\\\n",
    "  .& & \\\\\n",
    "  .& & \\\\\n",
    " \\frac{\\partial^2l(p, \\lambda)}{\\partial p_k\\partial p_1} & \\frac{\\partial^2l(p, \\lambda)}{\\partial p_k\\partial p_2} & ...\\frac{\\partial^2l(p, \\lambda)}{\\partial p_k^2}\\\\\n",
    "\\end{bmatrix}$$\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<dr> Substituting from (4),\n",
    "$$\\begin{bmatrix}\n",
    " \\frac{\\partial (\\frac{x_1}{p_1}-\\lambda)}{\\partial p_1} & \\frac{\\partial(\\frac{x_2}{p_2}-\\lambda)}{\\partial p_1} & ...\\frac{\\partial(\\frac{x_k}{p_k}-\\lambda)}{\\partial p_1} \\\\\n",
    " \n",
    " \\frac{\\partial (\\frac{x_1}{p_1}-\\lambda)}{\\partial p_2} & \\frac{\\partial(\\frac{x_2}{p_2}-\\lambda)}{\\partial p_2} & ...\\frac{\\partial(\\frac{x_k}{p_k}-\\lambda)}{\\partial p_2}\\\\\n",
    "  .& & \\\\\n",
    "  .& & \\\\\n",
    "  .& & \\\\\n",
    " \\frac{\\partial (\\frac{x_1}{p_1}-\\lambda)}{\\partial p_k} & \\frac{\\partial(\\frac{x_2}{p_2}-\\lambda)}{\\partial p_k} & ...\\frac{\\partial(\\frac{x_k}{p_k}-\\lambda)}{\\partial p_k}\\\\\n",
    "\\end{bmatrix}$$\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<dr> This now becomes,\n",
    "$$\\begin{bmatrix}\n",
    " \\frac{-x_1}{p_1^2} & 0 & ... 0 \\\\\n",
    " \n",
    " 0 & \\frac{-x_2}{p_2^2} & ... 0\\\\\n",
    "  .& & \\\\\n",
    "  .& & \\\\\n",
    "  .& & \\\\\n",
    " 0 & 0 & ... \\frac{-x_k}{p_k^2}\\\\\n",
    "\\end{bmatrix}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$As\\; x_1, x_2 ... x_k\\; are\\; all\\; whole\\; numbers,\\; \\geq 0\\; and \\; p_1, p_2..., p_k \\; are \\; probabilities \\; \\geq 0, $$\n",
    "$$\\frac{x_k}{p_k^2} \\geq 0$$\n",
    "$$\\frac{-x_k}{p_k^2} \\leq 0$$\n",
    "\n",
    "$$\\to The\\; given\\; matrix\\; is\\; NEGATIVE\\; SEMI\\; DEFINITE$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So the probabilities,\n",
    "$$p_k = \\frac{x_k}{\\lambda}$$\n",
    "$$where, \\; \\lambda = n\\;.... from \\; (5)$$\n",
    "\n",
    "are the required estimates for the MLE for the parameters of a k-sided multinomial distribution."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "interpreter": {
   "hash": "9e5e28c5b929416f221fb6605a701fda341e3fadbb2ce7c8964d3325eb15f30d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}