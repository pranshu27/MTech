{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08b91ad5",
   "metadata": {},
   "source": [
    "### Q1.a)  Use this function to find minima for (i) x2 + 3x+4 and (ii) x4 – 3x2 +2x. [5 points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "95728efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minima for the function x^2+3x+4 is found at -1.5\n",
      "Minima for the function x^4-3x^2+2x is found at 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "gradient1 = lambda x: 2*x+3 ## specifies gradient for the function x2 + 3x+4\n",
    "gradient2 = lambda x: 4*(x**3)-6*x+2 ## specifies gradient for the function  x4 – 3x2 +2x.\n",
    "\n",
    "def gradient_descent (gradient, init_, learn_rate, n_iter=1000, tol=1e-06):\n",
    "    x = init_\n",
    "    for i in range (n_iter): \n",
    "        delta = (-1)*learn_rate * gradient(x)\n",
    "        if np.all(np.abs (delta) <= tol): \n",
    "            break \n",
    "        x += delta \n",
    "    return  round(x*1000)/1000\n",
    "\n",
    "\n",
    "print('Minima for the function x^2+3x+4 is found at '+ str(gradient_descent(gradient1,4 , 0.1)))\n",
    "\n",
    "print('Minima for the function x^4-3x^2+2x is found at '+ str(gradient_descent(gradient2,4 , 0.01)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a7dec4",
   "metadata": {},
   "source": [
    "### Q1.b) Write a gradient function to calculate gradients for a linear regression y = ax + b [10 points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "287c9ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_gradients_calculator(X, Y, learning_rate = 0.01, init_a = 0, init_b = 0, max_iter = 30, tol = 1e-06):\n",
    "    a = 0\n",
    "    b = 0\n",
    "\n",
    "    n = float(len(X)) \n",
    "    \n",
    "    for i in range(max_iter): \n",
    "        Y_pred = a*X + b \n",
    "        \n",
    "        D_a = (-2/n) * sum(X * (Y - Y_pred))  # gradient wrt a\n",
    "        D_b = (-2/n) * sum(Y - Y_pred)  # gradient wrt b\n",
    "        \n",
    "        print('Gradient wrt \"a\": ', str(D_a), 'Gradient wrt \"b\": ', str(D_b))\n",
    "        \n",
    "        delta_a = (-1)*learning_rate * D_a\n",
    "        delta_b = (-1)*learning_rate * D_b\n",
    "\n",
    "        if (np.all(np.abs (delta_a) <= tol) and np.all(np.abs (delta_b) <= tol)): \n",
    "            break\n",
    "        \n",
    "        a+=delta_a\n",
    "        b+=delta_b\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bb989996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient wrt \"a\":  -10.732095840897884 Gradient wrt \"b\":  -4.905313437381427\n",
      "Gradient wrt \"a\":  -8.827398630157248 Gradient wrt \"b\":  -4.495135915979009\n",
      "Gradient wrt \"a\":  -7.247353142900043 Gradient wrt \"b\":  -4.148547328558394\n",
      "Gradient wrt \"a\":  -5.936807080137242 Gradient wrt \"b\":  -3.85483557168645\n",
      "Gradient wrt \"a\":  -4.849974863413731 Gradient wrt \"b\":  -3.6051065198660663\n",
      "Gradient wrt \"a\":  -3.948846887502027 Gradient wrt \"b\":  -3.3919752975366517\n",
      "Gradient wrt \"a\":  -3.201868928685552 Gradient wrt \"b\":  -3.209309981884463\n",
      "Gradient wrt \"a\":  -2.582845828290958 Gradient wrt \"b\":  -3.052018832176958\n",
      "Gradient wrt \"a\":  -2.070031362953744 Gradient wrt \"b\":  -2.9158736535452054\n",
      "Gradient wrt \"a\":  -1.6453726816498069 Gradient wrt \"b\":  -2.7973631585288707\n",
      "Gradient wrt \"a\":  -1.2938830595227675 Gradient wrt \"b\":  -2.6935712318872813\n",
      "Gradient wrt \"a\":  -1.003121176551481 Gradient wrt \"b\":  -2.6020758693756925\n",
      "Gradient wrt \"a\":  -0.7627588300156192 Gradient wrt \"b\":  -2.5208652794442905\n",
      "Gradient wrt \"a\":  -0.5642220621064 Gradient wrt \"b\":  -2.448268233095978\n",
      "Gradient wrt \"a\":  -0.40039323463727633 Gradient wrt \"b\":  -2.3828962421508764\n",
      "Gradient wrt \"a\":  -0.2653637002492161 Gradient wrt \"b\":  -2.3235955571107563\n",
      "Gradient wrt \"a\":  -0.15422847734139356 Gradient wrt \"b\":  -2.2694073169705975\n",
      "Gradient wrt \"a\":  -0.06291579526238811 Gradient wrt \"b\":  -2.219534466540536\n",
      "Gradient wrt \"a\":  0.011954412231539872 Gradient wrt \"b\":  -2.17331429196002\n",
      "Gradient wrt \"a\":  0.07318798151628614 Gradient wrt \"b\":  -2.130195620273269\n",
      "Gradient wrt \"a\":  0.12311404297714455 Gradient wrt \"b\":  -2.089719890974793\n",
      "Gradient wrt \"a\":  0.1636659824315641 Gradient wrt \"b\":  -2.0515054419538172\n",
      "Gradient wrt \"a\":  0.19644865288845523 Gradient wrt \"b\":  -2.01523446394171\n",
      "Gradient wrt \"a\":  0.2227941717406779 Gradient wrt \"b\":  -1.9806421702748864\n",
      "Gradient wrt \"a\":  0.2438082419194544 Gradient wrt \"b\":  -1.947507805750879\n",
      "Gradient wrt \"a\":  0.2604086063203674 Gradient wrt \"b\":  -1.9156471822482433\n",
      "Gradient wrt \"a\":  0.2733569715027069 Gradient wrt \"b\":  -1.8849064818240997\n",
      "Gradient wrt \"a\":  0.28328550977157113 Gradient wrt \"b\":  -1.8551571120369188\n",
      "Gradient wrt \"a\":  0.2907188603928559 Gradient wrt \"b\":  -1.8262914347987136\n",
      "Gradient wrt \"a\":  0.2960923943208595 Gradient wrt \"b\":  -1.798219220408678\n"
     ]
    }
   ],
   "source": [
    "\n",
    "linear_regression_gradients_calculator(X, Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62526eae",
   "metadata": {},
   "source": [
    "### Q1.c) Generate artificial data for this regression according to the following protocol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "914c499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np. random.seed (0)\n",
    "X = 2.5*np.random.randn(10000)+1.5 #Arra\n",
    "res=1.5*np.random.randn(10000) # Gene.\n",
    "Y = 2+0.3*X + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1d33faa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_gd(X, Y, learning_rate = 0.01, init_a = 0, init_b = 0, max_iter = 1000, tol = 1e-06):\n",
    "    start_time = time.time()\n",
    "\n",
    "    a = 0\n",
    "    b = 0\n",
    "\n",
    "    n = float(len(X)) \n",
    "    \n",
    "    for i in range(max_iter): \n",
    "        Y_pred = a*X + b \n",
    "        \n",
    "        D_a = (-2/n) * sum(X * (Y - Y_pred))  \n",
    "        D_b = (-2/n) * sum(Y - Y_pred)  \n",
    "        \n",
    "        delta_a = (-1)*learning_rate * D_a\n",
    "        delta_b = (-1)*learning_rate * D_b\n",
    "\n",
    "        if (np.all(np.abs (delta_a) <= tol) and np.all(np.abs (delta_b) <= tol)): \n",
    "            break\n",
    "        \n",
    "        a+=delta_a\n",
    "        b+=delta_b\n",
    "# =============================================================================\n",
    "#         print(i)\n",
    "# =============================================================================\n",
    "    print('It took ', str(i-1), ' iterations to arrive at the desired result')\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    return (round(a,2), round(b,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "136b9e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took  706  iterations to arrive at the desired result\n",
      "--- 1.0031802654266357 seconds ---\n",
      "\n",
      "Using batch gradient descent with a learning rate of 0.01, the values of (a,b) are  (0.3, 2.02)\n"
     ]
    }
   ],
   "source": [
    " print('\\nUsing batch gradient descent with a learning rate of 0.01, the values of (a,b) are ', str(lr_gd(X, Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6df6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97cf01e6",
   "metadata": {},
   "source": [
    "### Q1.d) Implement minibatch stochastic gradient descent using the code base you have developed so far.[15 points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f6349dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batches(ins, outs, bs): #a utility function to create minibatches of a specific size\n",
    "    for start_idx in range(0, ins.shape[0], bs):\n",
    "        end_idx = min(start_idx + bs, ins.shape[0])\n",
    "        ids = slice(start_idx, end_idx)\n",
    "        yield ins[ids], outs[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dc77fdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_mb_gd(X, Y, batch_size, learning_rate = 0.01, init_a = 0, init_b = 0, max_iter = 1000, tol = 1e-06):\n",
    "    start_time = time.time()\n",
    "    a = 0\n",
    "    b = 0\n",
    "\n",
    "    n = float(len(X)) \n",
    "    \n",
    "    for i in range(max_iter): \n",
    "        for mini_batch in  create_mini_batches(X, Y, batch_size):\n",
    "            \n",
    "            X_mini, Y_mini = mini_batch\n",
    "            Y_pred = a*X_mini + b \n",
    "            \n",
    "            D_a = (-2/n) * sum(X_mini * (Y_mini - Y_pred))  \n",
    "            D_b = (-2/n) * sum(Y_mini - Y_pred)  \n",
    "            \n",
    "            delta_a = (-1)*learning_rate * D_a\n",
    "            delta_b = (-1)*learning_rate * D_b\n",
    "    \n",
    "            if (np.all(np.abs (delta_a) <= tol) and np.all(np.abs (delta_b) <= tol)): \n",
    "                break\n",
    "            \n",
    "            a+=delta_a\n",
    "            b+=delta_b\n",
    "# =============================================================================\n",
    "#         print(i)\n",
    "# =============================================================================\n",
    "    print('It took ', str(i-1), ' iterations to arrive at the desired result')\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    return (round(a,2), round(b,2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfda96f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25efb2f3",
   "metadata": {},
   "source": [
    "> I will try various batch sizes(with a fixed learning rate __0.01__) and will finally pick a size that gives us the best values of _a_ and _b_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5d94dc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took  998  iterations to arrive at the desired result\n",
      "--- 2.2312686443328857 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4, 1.37)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_mb_gd(X, Y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "150857be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took  998  iterations to arrive at the desired result\n",
      "--- 2.825183868408203 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.32, 1.79)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_mb_gd(X, Y, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2c47b66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took  998  iterations to arrive at the desired result\n",
      "--- 4.123951196670532 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.29, 1.96)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_mb_gd(X, Y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a9657bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took  998  iterations to arrive at the desired result\n",
      "--- 4.612688302993774 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3, 1.97)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_mb_gd(X, Y, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "da0601cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took  998  iterations to arrive at the desired result\n",
      "--- 5.485988616943359 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3, 2.0)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_mb_gd(X, Y, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8a92cf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took  998  iterations to arrive at the desired result\n",
      "--- 4.2438743114471436 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3, 2.02)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_mb_gd(X, Y, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c0ffe68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took  998  iterations to arrive at the desired result\n",
      "--- 1.721069574356079 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3, 2.02)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_mb_gd(X, Y, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8cfdf611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took  998  iterations to arrive at the desired result\n",
      "--- 1.5840883255004883 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3, 2.02)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_mb_gd(X, Y, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1d5cc021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took  998  iterations to arrive at the desired result\n",
      "--- 1.4243991374969482 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3, 2.02)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_mb_gd(X, Y, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe44098",
   "metadata": {},
   "source": [
    "> As observed, the execution time keeps on decreasing as we increase the batch size, but we also see a slight deviation in the precision of the values a and b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a597c30",
   "metadata": {},
   "source": [
    "### Q1.e) Does SGD do better or worse in terms of time performance on our data? Is there an optimal minibatch size that works best? Quantify and interpret your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea2ac4b",
   "metadata": {},
   "source": [
    "> We tried different batch sizes for Stochastic Mini Batch GD and got the most optinum results for a size of __13__ (in terms of precision)\n",
    "\n",
    "### Batch gradient descent clearly seems to be a better option for doing linear regression\n",
    "1. It took __998__ iterations even with the most optimum batch size in Mini Batch Stochastic gradient descent where as it took __706__ in batch versison\n",
    "2. It took __~1s__ execution time for Batch GD whereas it took __~1.42s(Batch size=5000)__ for the other one\n",
    "\n",
    "#### INFERENCE: Clearly, SGD works poorly in terms of time performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330a056e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320f0a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
